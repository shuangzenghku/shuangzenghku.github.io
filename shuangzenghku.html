<table>
    <tr>
        <th>Date & Source & Link</th>
        <th>Paper</th>
    </tr>
    <tr>
        <td>
            <span style='display: inline-block; width: 42px;'>2025-02-06</span>
            <div style='min-width:85px;'>arxiv</div>
            <div style='min-width:85px;'><a href="http://arxiv.org/abs/2502.03792v1">Link</a></div>
        </td>
        <td>
            <strong>Guiding Two-Layer Neural Network Lipschitzness via Gradient Descent Learning Rate Constraints</strong><br>
            <sub>Author: Kyle Sung<br>
            我们展示了在经验风险最小化（ERM）中应用学习率（LR）的逐渐衰减，确保通过标准梯度下降（GD）训练的具有Lipschitz激活函数的两层神经网络展现出高水平的Lipschitz正则性，即较小的Lipschitz常数。此外，我们还表明，这种衰减不会妨碍以Huber损失测量的经验风险的收敛速率，朝向非凸经验风险的临界点。从这些发现中，我们推导出通过GD和逐渐衰减的LR训练的两层神经网络的泛化界限，且其对可训练参数数量的依赖呈次线性，表明这些网络的统计行为与过参数化无关。我们通过一系列玩具数值实验验证了我们的理论结果，令人惊讶的是，我们观察到使用恒定步长GD训练的网络展现出与使用逐渐衰减LR训练的网络类似的学习和正则性特征。这表明，使用标准GD训练的神经网络可能已经是高度正则的学习者。</sub>
        </td>
    </tr>
</table>
